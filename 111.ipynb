{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evasion Attacks against Machine Learning at Test Time\n"
     ]
    }
   ],
   "source": [
    "from services import PaperService\n",
    "PaperService.init()\n",
    "title = PaperService.search_papers(\"attack\")[0]['title']\n",
    "print (title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.86851275 0.929684   1.0865244  1.0894794 ]] [[     0   5190 138244  36139  89888]]\n",
      "[{'abstract': 'In security-sensitive applications, the success of machine '\n",
      "              'learning depends on a thorough vetting of their resistance to '\n",
      "              'adversarial data. In one pertinent, well-motivated attack '\n",
      "              'scenario, an adversary may attempt to evade a deployed system '\n",
      "              'at test time by carefully manipulating attack samples. In this '\n",
      "              'work, we present a simple but effective gradient-based approach '\n",
      "              'that can be exploited to systematically assess the security of '\n",
      "              'several, widely-used classification algorithms against evasion '\n",
      "              'attacks. Following a recently proposed framework for security '\n",
      "              'evaluation, we simulate attack scenarios that exhibit different '\n",
      "              \"risk levels for the classifier by increasing the attacker's \"\n",
      "              'knowledge of the system and her ability to manipulate attack '\n",
      "              'samples. This gives the classifier designer a better picture of '\n",
      "              'the classifier performance under evasion attacks, and allows '\n",
      "              'him to perform a more informed model selection (or parameter '\n",
      "              'setting). We evaluate our approach on the relevant security '\n",
      "              'task of malware detection in PDF files, and show that such '\n",
      "              'systems can be easily evaded. We also sketch some '\n",
      "              'countermeasures suggested by our analysis.',\n",
      "  'category': 'cs.CR',\n",
      "  'title': 'Evasion Attacks against Machine Learning at Test Time',\n",
      "  'year': 2013},\n",
      " {'abstract': 'Machine learning (ML) has made tremendous progress during the '\n",
      "              'past decade and is being adopted in various critical real-world '\n",
      "              'applications. However, recent research has shown that ML models '\n",
      "              'are vulnerable to multiple security and privacy attacks. In '\n",
      "              'particular, backdoor attacks against ML models that have '\n",
      "              'recently raised a lot of awareness. A successful backdoor '\n",
      "              'attack can cause severe consequences, such as allowing an '\n",
      "              'adversary to bypass critical authentication systems. '\n",
      "              '#R##N#Current backdooring techniques rely on adding static '\n",
      "              'triggers (with fixed patterns and locations) on ML model '\n",
      "              'inputs. In this paper, we propose the first class of dynamic '\n",
      "              'backdooring techniques: Random Backdoor, Backdoor Generating '\n",
      "              'Network (BaN), and conditional Backdoor Generating Network '\n",
      "              '(c-BaN). Triggers generated by our techniques can have random '\n",
      "              'patterns and locations, which reduce the efficacy of the '\n",
      "              'current backdoor detection mechanisms. In particular, BaN and '\n",
      "              'c-BaN are the first two schemes that algorithmically generate '\n",
      "              'triggers, which rely on a novel generative network. Moreover, '\n",
      "              'c-BaN is the first conditional backdooring technique, that '\n",
      "              'given a target label, it can generate a target-specific '\n",
      "              'trigger. Both BaN and c-BaN are essentially a general framework '\n",
      "              'which renders the adversary the flexibility for further '\n",
      "              'customizing backdoor attacks. #R##N#We extensively evaluate our '\n",
      "              'techniques on three benchmark datasets: MNIST, CelebA, and '\n",
      "              'CIFAR-10. Our techniques achieve almost perfect attack '\n",
      "              'performance on backdoored data with a negligible utility loss. '\n",
      "              'We further show that our techniques can bypass current '\n",
      "              'state-of-the-art defense mechanisms against backdoor attacks, '\n",
      "              'including Neural Cleanse, ABS, and STRIP.',\n",
      "  'category': 'unknown',\n",
      "  'title': 'Dynamic Backdoor Attacks against Machine Learning Models',\n",
      "  'year': 2020},\n",
      " {'abstract': 'Machine learning models are vulnerable to simple model stealing '\n",
      "              'attacks if the adversary can obtain output labels for chosen '\n",
      "              'inputs. To protect against these attacks, it has been proposed '\n",
      "              'to limit the information provided to the adversary by omitting '\n",
      "              'probability scores, significantly impacting the utility of the '\n",
      "              'provided service. In this work, we illustrate how a service '\n",
      "              'provider can still provide useful, albeit misleading, class '\n",
      "              'probability information, while significantly limiting the '\n",
      "              'success of the attack. Our defense forces the adversary to '\n",
      "              'discard the class probabilities, requiring significantly more '\n",
      "              'queries before they can train a model with comparable '\n",
      "              'performance. We evaluate several attack strategies, model '\n",
      "              'architectures, and hyperparameters under varying adversarial '\n",
      "              'models, and evaluate the efficacy of our defense against the '\n",
      "              'strongest adversary. Finally, we quantify the amount of noise '\n",
      "              'injected into the class probabilities to mesure the loss in '\n",
      "              'utility, e.g., adding 1.26 nats per query on CIFAR-10 and 3.27 '\n",
      "              'on MNIST. Our evaluation shows our defense can degrade the '\n",
      "              'accuracy of the stolen model at least 20%, or require up to 64 '\n",
      "              'times more queries while keeping the accuracy of the protected '\n",
      "              'model almost intact.',\n",
      "  'category': 'cs.LG',\n",
      "  'title': 'Defending against Machine Learning Model Stealing Attacks Using '\n",
      "           'Deceptive Perturbations',\n",
      "  'year': 2018},\n",
      " {'abstract': 'Machine learning is gaining popularity in the network security '\n",
      "              'domain as many more network-enabled devices get connected, as '\n",
      "              'malicious activities become stealthier, and as new technologies '\n",
      "              'like Software Defined Networking emerge. Compromising machine '\n",
      "              'learning model is a desirable goal. In fact, spammers have been '\n",
      "              'quite successful getting through machine learning enabled spam '\n",
      "              'filters for years. While previous works have been done on '\n",
      "              'adversarial machine learning, none has been considered within a '\n",
      "              'defense-in-depth environment, in which correct classification '\n",
      "              'alone may not be good enough. For the first time, this paper '\n",
      "              'proposes a cyber kill-chain for attacking machine learning '\n",
      "              'models together with a proof of concept. The intention is to '\n",
      "              'provide a high level attack model that inspire more secure '\n",
      "              'processes in research/design/implementation of machine learning '\n",
      "              'based security solutions.',\n",
      "  'category': 'cs.CR',\n",
      "  'title': 'Attacking Machine Learning Models As Part of a Cyber Kill Chain',\n",
      "  'year': 2017},\n",
      " {'abstract': 'Machine Learning (ML) has automated a multitude of our '\n",
      "              'day-to-day decision making domains such as education, '\n",
      "              'employment and driving automation. The continued success of ML '\n",
      "              'largely depends on our ability to trust the model we are using. '\n",
      "              'Recently, a new class of attacks called Backdoor Attacks have '\n",
      "              \"been developed. These attacks undermine the user's trust in ML \"\n",
      "              'models. In this work, we present NEO, a model agnostic '\n",
      "              'framework to detect and mitigate such backdoor attacks in image '\n",
      "              'classification ML models. For a given image classification '\n",
      "              'model, our approach analyses the inputs it receives and '\n",
      "              'determines if the model is backdoored. In addition to this '\n",
      "              'feature, we also mitigate these attacks by determining the '\n",
      "              'correct predictions of the poisoned images. An appealing '\n",
      "              'feature of NEO is that it can, for the first time, isolate and '\n",
      "              'reconstruct the backdoor trigger. NEO is also the first defence '\n",
      "              'methodology, to the best of our knowledge that is completely '\n",
      "              'blackbox. #R##N#We have implemented NEO and evaluated it '\n",
      "              'against three state of the art poisoned models. These models '\n",
      "              'include highly critical applications such as traffic sign '\n",
      "              'detection (USTS) and facial detection. In our evaluation, we '\n",
      "              'show that NEO can detect $\\\\approx$88\\\\% of the poisoned inputs '\n",
      "              'on average and it is as fast as 4.4 ms per input image. We also '\n",
      "              'reconstruct the poisoned input for the user to effectively test '\n",
      "              'their systems.',\n",
      "  'category': 'unknown',\n",
      "  'title': 'Model Agnostic Defence against Backdoor Attacks in Machine '\n",
      "           'Learning',\n",
      "  'year': 2019}]\n"
     ]
    }
   ],
   "source": [
    "papers =PaperService.get_similar_papers(title)\n",
    "import pprint\n",
    "pprint.pprint(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
